{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2393fd",
   "metadata": {},
   "source": [
    "# A single neuron to classify numbers in a MNSIT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab622407",
   "metadata": {},
   "source": [
    "## Logistic regression model = A single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b11d8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565086f",
   "metadata": {},
   "source": [
    "## Load the CSV file in form a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7bc6d586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b640d",
   "metadata": {},
   "source": [
    "## Converting the dataframe from pandas to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f65a49d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [7., 0., 0., ..., 0., 0., 0.],\n",
       "       [6., 0., 0., ..., 0., 0., 0.],\n",
       "       [9., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.to_numpy().astype(np.float32)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bbdf1",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b8cf893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c91680",
   "metadata": {},
   "source": [
    "## Split the data into input and output matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f44ed",
   "metadata": {},
   "source": [
    "### Note that although the data consists of images they are already flattened into 1D array. Therefore if you were to print out the shape of X you would see that it has the shape N by 784,  since 784 = 28 x 28. So you don't have to flattened anything yourself since the data is already flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c66d3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,1:] # input matrix\n",
    "Y = data[:,0] # Output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6def124d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "838d503d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000,)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e7703",
   "metadata": {},
   "source": [
    "## Split the data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "95533384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain : (33600, 784) ; Ytrain : (33600,) ; Xtest : (8400, 784) ; Ytest : (8400,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = X[:-8400]\n",
    "Ytrain = Y[:-8400]\n",
    "Xtest = X[-8400:]\n",
    "Ytest = Y[-8400:]\n",
    "print(f\"Xtrain : {Xtrain.shape} ; Ytrain : {Ytrain.shape} ; Xtest : {Xtest.shape} ; Ytest : {Ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23fb51",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a237c25",
   "metadata": {},
   "source": [
    "### We normalize the data specifically we use standardization, which means that every column of the data will have mean = 0 and variance = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612371c8",
   "metadata": {},
   "source": [
    "### since we want the mean along the rows we pass in axis = 0. we do the same for the standard diviation called std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb37d4",
   "metadata": {},
   "source": [
    "### You will notice that we called the mean and standard diviation on Xtrain instead of Xtest and not on the entire X matrix. This is because as you know the test set is supposed to represent the data that the model has never seen before. Therefore any preprocessing parameters we create cannot involve the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b3abee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = Xtrain.mean(axis=0)\n",
    "std = Xtrain.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0de81",
   "metadata": {},
   "source": [
    "## are they all zeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4686a",
   "metadata": {},
   "source": [
    "#### Next we note that in the case where the pixel values for all the images is the same , the variance is 0. This is because the variance of the constant is zero and therefore the standard diviation, which is the square root of the variance is also Zero. However, if you try to divide by the standard deviaiton in the next step, you would be dividing by zero which will give you infinity which is not desireable. What we would actually like to do is not change these values at all. If the value of some input is always zero, we should just let it remain a zero. Therefore we would effectively let you divide by 1. Thus we call the place function and we say anywhere where std is equal to zero replace it with one. Finally we do the standardization on both the train and test set. Standardization computation is done by subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "401409c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(std==0)[0]\n",
    "assert(np.all(std[idx]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dde34002",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.place(std,std==0,1)\n",
    "Xtrain = (Xtrain-mu)/std\n",
    "Xtest = (Xtest-mu)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b6737",
   "metadata": {},
   "source": [
    "## So how do you know your model is good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9322d4",
   "metadata": {},
   "source": [
    "#### Well you know if it's good, if it is better than something else, that something else is known as the benchmark. So here we will be using a single neuron as a benchmark. If we ever perform worst than this benchmark than we know there must be something wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b0b7c607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing logistic regression\n",
      "Iteration : 500 , Train loss : 11450.694728643664\n",
      "Train error : 0.09708333333333333 , Test loss : 3105.3362133350765\n",
      "Test error : 0.10404761904761904\n",
      "Iteration : 500 , Train loss : 10110.51090816952\n",
      "Train error : 0.08535714285714285 , Test loss : 2810.514778900044\n",
      "Test error : 0.09392857142857143\n",
      "Iteration : 500 , Train loss : 9526.930066335151\n",
      "Train error : 0.07919642857142857 , Test loss : 2699.591893337059\n",
      "Test error : 0.08976190476190476\n",
      "Iteration : 500 , Train loss : 9166.043733444683\n",
      "Train error : 0.07622023809523809 , Test loss : 2639.8955140561648\n",
      "Test error : 0.08678571428571429\n",
      "Iteration : 500 , Train loss : 8910.353167887792\n",
      "Train error : 0.07401785714285715 , Test loss : 2602.907135750945\n",
      "Test error : 0.08583333333333333\n",
      "Iteration : 500 , Train loss : 8714.3709727005\n",
      "Train error : 0.07235119047619047 , Test loss : 2578.070268199518\n",
      "Test error : 0.08523809523809524\n",
      "Iteration : 500 , Train loss : 8556.32343041267\n",
      "Train error : 0.07113095238095238 , Test loss : 2560.5895556663236\n",
      "Test error : 0.08511904761904762\n",
      "Iteration : 500 , Train loss : 8424.404946824725\n",
      "Train error : 0.07032738095238095 , Test loss : 2547.9101604579632\n",
      "Test error : 0.0830952380952381\n",
      "Iteration : 500 , Train loss : 8311.506697344526\n",
      "Train error : 0.06904761904761905 , Test loss : 2538.534139767882\n",
      "Test error : 0.08297619047619048\n",
      "Iteration : 500 , Train loss : 8213.023440482777\n",
      "Train error : 0.06806547619047619 , Test loss : 2531.5241900286874\n",
      "Test error : 0.0825\n",
      "Iteration : 500 , Train loss : 8125.809612680316\n",
      "Train error : 0.06720238095238096 , Test loss : 2526.264960869698\n",
      "Test error : 0.08190476190476191\n",
      "Iteration : 500 , Train loss : 8047.628025847406\n",
      "Train error : 0.06663690476190476 , Test loss : 2522.3383013706134\n",
      "Test error : 0.08202380952380953\n",
      "Iteration : 500 , Train loss : 7976.835255230746\n",
      "Train error : 0.06595238095238096 , Test loss : 2519.4526851823966\n",
      "Test error : 0.08154761904761905\n",
      "Iteration : 500 , Train loss : 7912.191107340359\n",
      "Train error : 0.06523809523809523 , Test loss : 2517.3993424089517\n",
      "Test error : 0.08154761904761905\n",
      "Iteration : 500 , Train loss : 7852.737930448486\n",
      "Train error : 0.06470238095238096 , Test loss : 2516.0224480383185\n",
      "Test error : 0.08142857142857143\n",
      "Iteration : 500 , Train loss : 7797.721532668084\n",
      "Train error : 0.06422619047619048 , Test loss : 2515.1987656253373\n",
      "Test error : 0.0805952380952381\n",
      "Iteration : 500 , Train loss : 7746.537916308735\n",
      "Train error : 0.06369047619047619 , Test loss : 2514.825620465612\n",
      "Test error : 0.0805952380952381\n",
      "Iteration : 500 , Train loss : 7698.6964500479535\n",
      "Train error : 0.06327380952380952 , Test loss : 2514.8157290257573\n",
      "Test error : 0.08047619047619048\n",
      "Iteration : 500 , Train loss : 7653.793713747036\n",
      "Train error : 0.06270833333333334 , Test loss : 2515.095915329106\n",
      "Test error : 0.0805952380952381\n",
      "Iteration : 500 , Train loss : 7611.494437163086\n",
      "Train error : 0.062232142857142854 , Test loss : 2515.6067174689024\n",
      "Test error : 0.0805952380952381\n",
      "Iteration : 500 , Train loss : 7571.517298139265\n",
      "Train error : 0.061785714285714284 , Test loss : 2516.3013271913956\n",
      "Test error : 0.08083333333333333\n",
      "Iteration : 500 , Train loss : 7533.624155961338\n",
      "Train error : 0.06169642857142857 , Test loss : 2517.1437336370145\n",
      "Test error : 0.08071428571428571\n",
      "Iteration : 500 , Train loss : 7497.61177818489\n",
      "Train error : 0.06136904761904762 , Test loss : 2518.1065493084066\n",
      "Test error : 0.08047619047619048\n",
      "Iteration : 500 , Train loss : 7463.305413010669\n",
      "Train error : 0.06107142857142857 , Test loss : 2519.1689666997795\n",
      "Test error : 0.08023809523809523\n",
      "Iteration : 500 , Train loss : 7430.553746815743\n",
      "Train error : 0.060654761904761906 , Test loss : 2520.3150671236563\n",
      "Test error : 0.08023809523809523\n",
      "Iteration : 500 , Train loss : 7399.224912431181\n",
      "Train error : 0.060476190476190475 , Test loss : 2521.532521708366\n",
      "Test error : 0.08023809523809523\n",
      "Iteration : 500 , Train loss : 7369.203301752629\n",
      "Train error : 0.06032738095238095 , Test loss : 2522.811635156987\n",
      "Test error : 0.08035714285714286\n",
      "Iteration : 500 , Train loss : 7340.386999267357\n",
      "Train error : 0.06014880952380952 , Test loss : 2524.1446571924275\n",
      "Test error : 0.0805952380952381\n",
      "Iteration : 500 , Train loss : 7312.685698857345\n",
      "Train error : 0.06005952380952381 , Test loss : 2525.525291077362\n",
      "Test error : 0.08035714285714286\n",
      "Iteration : 500 , Train loss : 7286.018999823393\n",
      "Train error : 0.05976190476190476 , Test loss : 2526.9483427638693\n",
      "Test error : 0.08\n",
      "Iteration : 500 , Train loss : 7260.315002895923\n",
      "Train error : 0.05949404761904762 , Test loss : 2528.409468865562\n",
      "Test error : 0.07988095238095239\n",
      "Iteration : 500 , Train loss : 7235.509145454818\n",
      "Train error : 0.05949404761904762 , Test loss : 2529.9049936617535\n",
      "Test error : 0.07988095238095239\n",
      "Iteration : 500 , Train loss : 7211.543228983463\n",
      "Train error : 0.05931547619047619 , Test loss : 2531.431774309165\n",
      "Test error : 0.08023809523809523\n",
      "Iteration : 500 , Train loss : 7188.364602163655\n",
      "Train error : 0.059226190476190474 , Test loss : 2532.987099829542\n",
      "Test error : 0.08023809523809523\n",
      "Iteration : 500 , Train loss : 7165.925470872893\n",
      "Train error : 0.05919642857142857 , Test loss : 2534.5686139071963\n",
      "Test error : 0.08023809523809523\n",
      "Iteration : 500 , Train loss : 7144.182312329078\n",
      "Train error : 0.05904761904761905 , Test loss : 2536.174254628579\n",
      "Test error : 0.08035714285714286\n",
      "Iteration : 500 , Train loss : 7123.095375220527\n",
      "Train error : 0.05880952380952381 , Test loss : 2537.8022064442944\n",
      "Test error : 0.08035714285714286\n",
      "Iteration : 500 , Train loss : 7102.62825121527\n",
      "Train error : 0.05860119047619047 , Test loss : 2539.4508611243004\n",
      "Test error : 0.08035714285714286\n",
      "Iteration : 500 , Train loss : 7082.747506022948\n",
      "Train error : 0.05842261904761905 , Test loss : 2541.118785506637\n",
      "Test error : 0.08023809523809523\n",
      "Iteration : 500 , Train loss : 7063.422360375379\n",
      "Train error : 0.05842261904761905 , Test loss : 2542.8046945429955\n",
      "Test error : 0.07952380952380952\n",
      "Iteration : 500 , Train loss : 7044.624413037344\n",
      "Train error : 0.058363095238095235 , Test loss : 2544.507428613926\n",
      "Test error : 0.07952380952380952\n",
      "Iteration : 500 , Train loss : 7026.327399359257\n",
      "Train error : 0.05839285714285714 , Test loss : 2546.225934390224\n",
      "Test error : 0.07952380952380952\n",
      "Iteration : 500 , Train loss : 7008.506980012833\n",
      "Train error : 0.05830357142857143 , Test loss : 2547.959248706001\n",
      "Test error : 0.07952380952380952\n",
      "Iteration : 500 , Train loss : 6991.14055546677\n",
      "Train error : 0.05821428571428571 , Test loss : 2549.7064850220654\n",
      "Test error : 0.07988095238095239\n",
      "Iteration : 500 , Train loss : 6974.20710250353\n",
      "Train error : 0.05803571428571429 , Test loss : 2551.466822124685\n",
      "Test error : 0.07976190476190476\n",
      "Iteration : 500 , Train loss : 6957.687029685711\n",
      "Train error : 0.05794642857142857 , Test loss : 2553.239494745676\n",
      "Test error : 0.07964285714285714\n",
      "Iteration : 500 , Train loss : 6941.562049176398\n",
      "Train error : 0.057708333333333334 , Test loss : 2555.0237858187475\n",
      "Test error : 0.07952380952380952\n",
      "Iteration : 500 , Train loss : 6925.815062725136\n",
      "Train error : 0.05738095238095238 , Test loss : 2556.819020112651\n",
      "Test error : 0.07964285714285714\n",
      "Iteration : 500 , Train loss : 6910.430059966019\n",
      "Train error : 0.05738095238095238 , Test loss : 2558.62455900761\n",
      "Test error : 0.07940476190476191\n",
      "Iteration : 500 , Train loss : 6895.392027451783\n",
      "Train error : 0.05717261904761905 , Test loss : 2560.43979620915\n",
      "Test error : 0.07940476190476191\n",
      "Final Benchmark model error rate: 0.07940476190476191\n"
     ]
    }
   ],
   "source": [
    "#utrns target matrix into an indicator matrix\n",
    "def y2indicator(y):\n",
    "    #grab N which is the number of samples \n",
    "    # N is the length of Y\n",
    "    N = len(y)\n",
    "    \n",
    "    # convert Y into an array of integers\n",
    "    # This is necessary since we will be using these values as an array of indicies\n",
    "    y = y.astype(np.int32)\n",
    "    \n",
    "    # Next we grab K the total number of classes\n",
    "    # Notice this depends on the targets being labeled correctly\n",
    "    # Specifically we would like them to be labeled from zero up to K-1\n",
    "    # That's why when we add 1 to the max we get K\n",
    "    K = y.max() + 1\n",
    "    \n",
    "    # Now we create our indicator matrix\n",
    "    # Indicator matrix will be initialized to the 2D array of zeros of shape (N x K)\n",
    "    ind = np.zeros((N, K))\n",
    "    \n",
    "    # Next we loop through each sample index from 0 to N-1\n",
    "    # Inside the loop, we assign a 1 to row N in column K\n",
    "    # This is ofcourse the K here is the target for the Nth sample\n",
    "    # return the indicator matrix after it has been populated at the end of the forloop\n",
    "    for n in range(N):\n",
    "        k = y[n]\n",
    "        ind[n, k] = 1\n",
    "    return ind\n",
    "\n",
    "# This function takes some input and make some model predictions\n",
    "def forward(X,W,b):\n",
    "    #softmax\n",
    "    # First we apply Linear transformation using W and b \n",
    "    a = X.dot(W) + b\n",
    "    # Second we apply softmax activation so that the output is the set of probabilities \n",
    "    expa = np.exp(a)\n",
    "    y = expa / expa.sum(axis=1, keepdims = True)\n",
    "    return y\n",
    "\n",
    "# This function takes an input the one hot encoded targets called T and the model outputs p_y\n",
    "# p_y is a short form of p of y given x\n",
    "# Code implements the categorical cross entropy loss function\n",
    "def cost(p_y,t):\n",
    "    total = t * np.log(p_y)\n",
    "    return -total.sum()\n",
    "\n",
    "# The predict funtion takes the output probabilities from a model and converts them \n",
    "# into class predictions\n",
    "# The classes must be integers from 0 to K-1\n",
    "# This involves taking the Argmax of the probabilities along the columns\n",
    "# Inorder for this to work the classes must be integers from zero up to K-1\n",
    "def predict(p_y):\n",
    "    return np.argmax(p_y, axis=1)\n",
    "\n",
    "# This function compares the prediction made by the model and compares it to the target\n",
    "def error_rate(p_y,t):\n",
    "    prediction = predict(p_y)  \n",
    "    '''\n",
    "    By using != operator we will get a boolean array.\n",
    "    Boolean array contains True if the predictions are not equal to the target and False \n",
    "    otherwise.\n",
    "    Now we will take the mean of this array. In python we have 1 if the statement is True \n",
    "    and 0 if the statement is False.\n",
    "    We have 1 every time the prediction is incorrect and a 0 every time the prediction is \n",
    "    correct.\n",
    "    So this is the same thing as summing up all the incorrect predictions and dividing by the\n",
    "    total number of predictions\n",
    "    Hence this returns the proportions of predictions that we got wrong\n",
    "    '''\n",
    "    return np.mean(prediction != t)\n",
    "\n",
    "# gradient ascent functions for derivative calculations for W and b\n",
    "def gradW(t,y,X):\n",
    "    return X.T.dot(t-y)\n",
    "def gradb(t,y):\n",
    "    return (t-y).sum(axis=0)\n",
    "\n",
    "trainL = []\n",
    "testL = []\n",
    "trainCE = []\n",
    "testCE = []\n",
    "def linear_benchmark():\n",
    "    print(\"Performing logistic regression\")\n",
    "    # grabbing the shape of our data matrix by calling Xtrain.shape this will give us \n",
    "    # back N and D . N = Number of samples and D = Number of features\n",
    "    N , D = Xtrain.shape\n",
    "    \n",
    "    # convert the Ytrain and Ytest to (N x K) matrices of the indicator variable\n",
    "    # In order to do multiclass classification we need to convert our targets into \n",
    "    # indicator matrices\n",
    "    Ytrain_indicator = y2indicator(Ytrain)\n",
    "    Ytest_indicator = y2indicator(Ytest)\n",
    "    \n",
    "    # Initialize our parameters in preparation for training\n",
    "    # Get the K which is the number of columns for the indicator matrix\n",
    "    K = Ytrain_indicator.shape[1]\n",
    "    # Then we initialize W to be a random matrix of size (D x K)\n",
    "    # Note that we draw the values from the standard normal, \n",
    "    # but then we divide by the square_root of D\n",
    "    # This is because we want the initial values to be small\n",
    "    W = np.random.randn(D, K) / np.sqrt(D)\n",
    "    # initialize the bias vector b which is the vector of zeros of length K\n",
    "    b = np.zeros(K)\n",
    "    \n",
    "    # Next we create empty lists to store some data while we train.\n",
    "    # Specifically we would like to store the loss for both the train and test sets\n",
    "    # as well as the classification_errors for both the train and test sets\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_classification_errors = []\n",
    "    test_classification_errors = []\n",
    "    \n",
    "    '''\n",
    "    This is simply some log of different Hyper parameters I tried when creating this code\n",
    "    Trying to find a good learning rate and regularization penalty is simply often simply a \n",
    "    matter of trial and error.\n",
    "    reg = 1\n",
    "    learning rate 0.0001 is too high, 0.00005 is also too high\n",
    "    0.00003/2000 iterations -> 0.363 error, -7630 cost\n",
    "    0.00004/1000 iterations -> 0.295 error, -7902 cost\n",
    "    0.00004/2000 iterations -> 0.321 error, -7528 cost\n",
    "    \n",
    "    reg = 0.1 still around 0.31 error\n",
    "    reg = 0.01 still around 0.31 error\n",
    "    '''\n",
    "    # Before we start the loop we are going to initialize the learning rate, regularization and \n",
    "    # number of iterations\n",
    "    lr = 0.00003\n",
    "    reg = 0.0\n",
    "    n_iters = 500\n",
    "    for i in range(n_iters):\n",
    "        p_y = forward(Xtrain, W, b)\n",
    "        train_loss = cost(p_y, Ytrain_indicator)\n",
    "        train_losses.append(train_loss)\n",
    "        trainL.append(train_loss)\n",
    "        \n",
    "        train_err = error_rate(p_y, Ytrain)\n",
    "        train_classification_errors.append(train_err)\n",
    "        trainCE.append(train_err)\n",
    "        \n",
    "        p_y_test = forward(Xtest, W, b)\n",
    "        test_loss = cost(p_y_test, Ytest_indicator)\n",
    "        test_losses.append(test_loss)\n",
    "        testL.append(test_loss)\n",
    "        \n",
    "        test_err = error_rate(p_y_test, Ytest)\n",
    "        test_classification_errors.append(test_err)\n",
    "        testCE.append(test_err)\n",
    "        \n",
    "        # gradient ascent\n",
    "        W += lr*(gradW(Ytrain_indicator, p_y, Xtrain) - reg*W)\n",
    "        b += lr*(gradb(Ytrain_indicator, p_y))\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Iteration : {n_iters} , Train loss : {train_loss}\")\n",
    "            print(f\"Train error : {train_err} , Test loss : {test_loss}\")\n",
    "            print(f\"Test error : {test_err}\")\n",
    "    # one final pass through the neuron to get our test predictions  \n",
    "    p_y = forward(Xtest,W,b)\n",
    "    print(f\"Final Benchmark model error rate: {error_rate(p_y,Ytest)}\")\n",
    "    \n",
    "\n",
    "linear_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "de984d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "legend1, = plt.plot(trainL)\n",
    "legend2, = plt.plot(testL)\n",
    "plt.legend([legend1,legend2],[\"Train Loss\", \"Test Loss\"], title = \"Loss per iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e0751da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "legend1, = plt.plot(trainCE)\n",
    "legend2, = plt.plot(testCE)\n",
    "plt.legend([legend1,legend2],[\"Train Error\", \"Test Error\"], title = \"Classification Error per iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01eef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
